# Integration tests for Korean analysis components
#
---
"Analyzer":
    - do:
        indices.analyze:
          body:
            text:         뿌리가 깊은 나무
            analyzer:     custom
    - length: { tokens: 3 }
    - match:  { tokens.0.token: 뿌리 }
    - match:  { tokens.1.token: 깊 }
    - match:  { tokens.2.token: 나무 }
---
"Tokenizer":
    - do:
        indices.analyze:
          body:
            text:         뿌리가 깊은 나무
            tokenizer:    custom_tokenizer
    - length: { tokens: 5 }
    - match:  { tokens.0.token: 뿌리 }
    - match:  { tokens.1.token: 가  }
    - match:  { tokens.2.token: 깊  }
    - match:  { tokens.3.token: 은  }
    - match:  { tokens.4.token: 나무 }
---
"Part of speech filter":
    - do:
        indices.analyze:
          body:
            text:         뿌리가 깊은 나무
            tokenizer:    custom_tokenizer
            filter:       [custom_part_of_speech]
    - length: { tokens: 3 }
    - match:  { tokens.0.token: 뿌리 }
    - match:  { tokens.1.token: 깊  }
    - match:  { tokens.2.token: 나무 }
---
"Reading filter":
    - do:
        indices.analyze:
          body:
            text:         鄕歌
            tokenizer:    custom_tokenizer
            filter:       [custom_readingform]
    - length: { tokens: 1 }
    - match:  { tokens.0.token: 향가 }
---
"Number filter":
    - do:
        indices.analyze:
          body:
            text:                   십만이천오백과 ３.２천
            tokenizer:
              type:                 custom_tokenizer
              discard_punctuation:  false
            filter:
              - type:               custom_part_of_speech
                stoptags:           ["SP"]
              - type:               custom_number
    - length: { tokens: 3 }
    - match:  { tokens.0.token: "102500"}
    - match:  { tokens.1.token: 과}
    - match:  { tokens.2.token: "3200"}
